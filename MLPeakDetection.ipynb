{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4991d73f-6b96-48e8-86ce-4439aaf57e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal, interpolate\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "import os \n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from bishop_peaks_onsets import bishop_peaks_and_onsets\n",
    "os.chdir('/home/kkotzen/research/PPG_sleepstaging_orion2 /')\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from src.parsing.MESAParser import MESAParser\n",
    "from scipy.signal import butter, sosfiltfilt, resample\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6700abd8-837d-4886-aec1-0274c0cda9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl1 = MESAParser()\n",
    "# patients = dl1.database_all_patient_IDs\n",
    "# peaks_path = \"/home/kkotzen/leas_bishop_peaks/peaks\"\n",
    "# onsets_path = \"/home/kkotzen/leas_bishop_peaks/onsets\"\n",
    "\n",
    "# fs = 256\n",
    "# filter_lowcut=0.4\n",
    "# filter_highcut=8\n",
    "# filter_order=8\n",
    "# sos = butter(filter_order, [filter_lowcut / (fs / 2), filter_highcut / (fs / 2)], 'bandpass', output='sos')\n",
    "\n",
    "# def extract(patient):\n",
    "#     try:\n",
    "#         dl = MESAParser()\n",
    "#         ppg = dl.load_signal(patient, 'Pleth')\n",
    "\n",
    "#         ppg_filt = sosfiltfilt(sos, ppg, axis=0)  # , padlen=3 * (max(len(b), len(a)) - 1))\n",
    "\n",
    "#         ppg_peaks, ppg_onsets = bishop_peaks_and_onsets(ppg_filt, fs=256, min_time_between_ms=20,\n",
    "#                                     pre_filter=True, filter_lowcut=0.4, filter_highcut=7.99, filter_order=8,\n",
    "#                                     resample=True, resample_fs=32,\n",
    "#                                     window=100, hop=90, correct_points=True)\n",
    "#         np.save(f\"{peaks_path}/{patient}.npy\", ppg_peaks)\n",
    "#         np.save(f\"{onsets_path}/{patient}.npy\", ppg_onsets)\n",
    "#     except:\n",
    "#         print(f\"Failed to process: {patient}\")\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# with Pool(64) as p:\n",
    "#         p.map(extract, patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0111ca-d0a4-4aa3-aa23-c3b5e05ad88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# import numpy as np\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# class PPGPeakOnsetLoader(keras.utils.Sequence):\n",
    "#     \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "    \n",
    "#     def __init__(self, batch_size, duration, hop, ppg_path, peak_path, patients):\n",
    "        \n",
    "#         self.fs = 256\n",
    "#         self.resample_fs = 32\n",
    "#         self.F = self.fs / self.resample_fs\n",
    "#         self.n_pool = 32\n",
    "        \n",
    "#         filter_lowcut=0.4\n",
    "#         filter_highcut=8\n",
    "#         filter_order=8\n",
    "#         self.sos = butter(filter_order, [filter_lowcut / (self.fs / 2), filter_highcut / (self.fs / 2)], 'bandpass', output='sos')\n",
    "        \n",
    "#         self.ppg_path = ppg_path\n",
    "#         self.peak_path = peak_path\n",
    "#         self.batch_size = batch_size\n",
    "#         self.window = duration\n",
    "#         self.hop = hop\n",
    "#         self.patients = patients\n",
    "        \n",
    "#         self.data = self._load_data()\n",
    "#         print(\"Datal loaded...\")\n",
    "#         self.labels = self._load_labels()\n",
    "#         print(\"Labels loaded...\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         samples = 0\n",
    "#         for patient in self.patients:\n",
    "#             samples += int(self.data[patient].shape[0]/(self.hop*self.fs))\n",
    "#         return samples // self.batch_size\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "#         i = idx * self.batch_size\n",
    "#         batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "#         batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "#         x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "#         for j, path in enumerate(batch_input_img_paths):\n",
    "#             img = load_img(path, target_size=self.img_size)\n",
    "#             x[j] = img\n",
    "#         y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "#         for j, path in enumerate(batch_target_img_paths):\n",
    "#             img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "#             y[j] = np.expand_dims(img, 2)\n",
    "#             # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "#             y[j] -= 1\n",
    "#         return x, y\n",
    "    \n",
    "    \n",
    "#     def _load_single_data(self, patient):\n",
    "#         ppg = np.load(f\"{self.ppg_path}/mesa-sleep-{patient}-Pleth.npy\")\n",
    "#         ppg = sosfiltfilt(self.sos, ppg, axis=0)\n",
    "#         F = self.fs / self.resample_fs\n",
    "#         ppg = scipy.signal.resample(ppg, int(len(ppg) / F)).astype(np.float16)\n",
    "#         return ppg\n",
    "    \n",
    "#     def _load_single_label(self, patient):\n",
    "#         peaks = np.round(np.load(f\"{self.peak_path}/{patient}.npy\")/self.F).astype(int)\n",
    "#         return peaks\n",
    "    \n",
    "#     def _load_data(self):\n",
    "#         data = Parallel(n_jobs=64, prefer=\"threads\")(delayed(self._load_single_data)(patient) for patient in tqdm.tqdm(self.patients))\n",
    "#         return data\n",
    "        \n",
    "#     def _load_labels(self):\n",
    "#         labels = [self._load_single_label(patient) for patient in tqdm.tqdm(self.patients)]\n",
    "#         return labels\n",
    "    \n",
    "#     def _make_dataset_from_signal_and_labels(self):\n",
    "#         window = self.resample_fs*self.window\n",
    "#         hop = self.resample_fs*self.hop\n",
    "#         shape_estimate = int(len(self.patients)*(11*60*60*32)/hop)\n",
    "#         print(shape_estimate)\n",
    "#         data = np.empty((shape_estimate,window), np.float16)\n",
    "#         labels = np.empty((shape_estimate,window), int)\n",
    "        \n",
    "#         shape = 0\n",
    "#         for i,patient in tqdm.tqdm(enumerate(self.patients)):\n",
    "#             patient_data = self.data[i]\n",
    "#             patient_labels = np.zeros_like(patient_data)\n",
    "#             patient_labels[self.labels[i][:-2]]=1\n",
    "            \n",
    "#             strided_data = self._time_series_subsequences(patient_data, window, hop)\n",
    "#             strided_labels = self._time_series_subsequences(patient_labels, window, hop)\n",
    "            \n",
    "#             data[shape:shape+strided_data.shape[0],:] = strided_data\n",
    "#             labels[shape:shape+strided_data.shape[0],:] = strided_labels\n",
    "#             shape = shape + strided_data.shape[0]\n",
    "#             if shape>shape_estimate:\n",
    "#                 print(\"Exceeded size\")\n",
    "#                 break\n",
    "        \n",
    "#         print(shape_estimate, shape, shape/shape_estimate)\n",
    "#         return data[0:shape], labels[0:shape]\n",
    "    \n",
    "\n",
    "#     def _time_series_subsequences(self,ts, window, hop=1):\n",
    "#         assert len(ts.shape) == 1\n",
    "#         shape = (int(int(ts.size - window) / hop + 1), window)\n",
    "#         strides = ts.strides[0] * hop, ts.strides[0]\n",
    "#         return np.lib.stride_tricks.as_strided(ts, shape=shape, strides=strides)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fda4184-8368-46bc-b528-f7521098b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppg_path = \"/home/kkotzen/databases/mesa/polysomnography/signals/npy/Pleth\"\n",
    "# peak_path = \"/home/kkotzen/leas_bishop_peaks/peaks\"\n",
    "\n",
    "# train_patients = [p.split(\".\")[0] for p in os.listdir(peak_path)][0:1000]\n",
    "# validate_patients = [p.split(\".\")[0] for p in os.listdir(peak_path)][1001:1200]\n",
    "# test_patients = [p.split(\".\")[0] for p in os.listdir(peak_path)][1201:1300]\n",
    "\n",
    "# train_dataloader = PPGPeakOnsetLoader(32, 16, 6, ppg_path, peak_path, train_patients)\n",
    "# d_train, l_train = train_dataloader._make_dataset_from_signal_and_labels()\n",
    "\n",
    "# validate_dataloader = PPGPeakOnsetLoader(32, 16, 6, ppg_path, peak_path, validate_patients)\n",
    "# d_validate, l_validate = validate_dataloader._make_dataset_from_signal_and_labels()\n",
    "\n",
    "# test_dataloader = PPGPeakOnsetLoader(32, 16, 6, ppg_path, peak_path, test_patients)\n",
    "# d_test, l_test = test_dataloader._make_dataset_from_signal_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfe890b-72ba-4dbb-be30-1dfc7f326943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"/tmp/deep_peaks_signals_train.npy\", d_train)\n",
    "# np.save(\"/tmp/deep_peaks_labels_train.npy\", l_train.astype(np.int8))\n",
    "\n",
    "# np.save(\"/tmp/deep_peaks_signals_validate.npy\", d_validate)\n",
    "# np.save(\"/tmp/deep_peaks_labels_validate.npy\", l_validate.astype(np.int8))\n",
    "\n",
    "# np.save(\"/tmp/deep_peaks_signals_test.npy\", d_test)\n",
    "# np.save(\"/tmp/deep_peaks_labels_test.npy\", l_test.astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffe27cb-e86b-48c9-ab78-1a264da80ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "d_train = np.load(\"/tmp/deep_peaks_signals_train.npy\")\n",
    "l_train = np.load(\"/tmp/deep_peaks_labels_train.npy\")\n",
    "\n",
    "d_validate = np.load(\"/tmp/deep_peaks_signals_validate.npy\")\n",
    "l_validate = np.load(\"/tmp/deep_peaks_labels_validate.npy\")\n",
    "\n",
    "d_test= np.load(\"/tmp/deep_peaks_signals_test.npy\")\n",
    "l_test = np.load(\"/tmp/deep_peaks_labels_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5721c1-a950-425c-b857-1054b9e0c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close('all')\n",
    "# plt.plot(d_train[500])\n",
    "# plt.plot(np.where(l_train[500]==1)[0], d_train[500][np.where(l_train[500]==1)[0]], 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae61ced-68a4-4ab8-a3e5-715b6dba381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 01:33:54.063057: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 01:33:57.878018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-04 01:33:59.069226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-08-04 01:33:59.069295: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 01:33:59.087759: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-04 01:33:59.087821: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-04 01:33:59.090671: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-04 01:33:59.093548: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-04 01:33:59.095337: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-08-04 01:33:59.097698: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-08-04 01:33:59.098369: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-08-04 01:33:59.219019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#Before ever importing tensorflow \n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set the GPU you wish to use here\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], enable=True) #set the GPU you want to use here\n",
    "print(gpus)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n",
    "from tensorflow.keras.initializers import random_normal\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "def cbr(x, out_layer, kernel, stride, dilation):\n",
    "    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def se_block(x_in, layer_n):\n",
    "    x = GlobalAveragePooling1D()(x_in)\n",
    "    x = Dense(layer_n//8, activation=\"relu\")(x)\n",
    "    x = Dense(layer_n, activation=\"sigmoid\")(x)\n",
    "    x_out=Multiply()([x_in, x])\n",
    "    return x_out\n",
    "\n",
    "def resblock(x_in, layer_n, kernel, dilation, use_se=True):\n",
    "    x = cbr(x_in, layer_n, kernel, 1, dilation)\n",
    "    x = cbr(x, layer_n, kernel, 1, dilation)\n",
    "    if use_se:\n",
    "        x = se_block(x, layer_n)\n",
    "    x = Add()([x_in, x])\n",
    "    return x  \n",
    "\n",
    "def Unet(input_shape=(512,1)):\n",
    "    layer_n = 32\n",
    "    kernel_size = 7\n",
    "    depth = 2\n",
    "\n",
    "    input_layer = Input(input_shape)\n",
    "    print('Input', input_layer.shape)\n",
    "    input_layer_1 = AveragePooling1D(4)(input_layer)\n",
    "    input_layer_2 = AveragePooling1D(16)(input_layer)\n",
    "    \n",
    "    ########## Encoder\n",
    "    x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n, kernel_size, 1)\n",
    "    out_0 = x\n",
    "\n",
    "    x = cbr(x, layer_n*2, kernel_size, 4, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*2, kernel_size, 1)\n",
    "    out_1 = x\n",
    "    x = Concatenate()([x, input_layer_1])    \n",
    "    x = cbr(x, layer_n*3, kernel_size, 4, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*3, kernel_size, 1)\n",
    "    out_2 = x\n",
    "    x = Concatenate()([x, input_layer_2])    \n",
    "    x = cbr(x, layer_n*4, kernel_size, 4, 1)\n",
    "    for i in range(depth):\n",
    "        x = resblock(x, layer_n*4, kernel_size, 1)\n",
    "    ########### Decoder\n",
    "    x = UpSampling1D(4)(x)\n",
    "    x = Concatenate()([x, out_2])\n",
    "    x = cbr(x, layer_n*3, kernel_size, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(4)(x)\n",
    "    x = Concatenate()([x, out_1])\n",
    "    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n",
    "\n",
    "    x = UpSampling1D(4)(x)\n",
    "    x = Concatenate()([x, out_0])\n",
    "    x = cbr(x, layer_n, kernel_size, 1, 1)    \n",
    "\n",
    "    #regressor\n",
    "    #x = Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
    "    #out = Activation(\"sigmoid\")(x)\n",
    "    #out = Lambda(lambda x: 12*x)(out)\n",
    "    \n",
    "    #classifier\n",
    "    x = Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
    "    out = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(input_layer, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9c3b4b-9aa7-41ee-ba1d-e2247e654b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n",
    "    \n",
    "    idx_1 = np.arange(len(input_dataset))\n",
    "    while True:\n",
    "        np.random.shuffle(idx_1)\n",
    "        for i in range(int(len(input_dataset)/batch_size)):\n",
    "            inputs = input_dataset[idx_1[i*batch_size:(i+1)*batch_size]]\n",
    "            targets = target_dataset[idx_1[i*batch_size:(i+1)*batch_size]]\n",
    "            yield inputs, targets\n",
    "\n",
    "class macroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "        f1_val = f1_score(self.targets, pred, average=\"macro\")\n",
    "        print(\"val_f1_macro_score: \", f1_val)\n",
    "                \n",
    "def model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n",
    "    print(f\"{train_inputs.shape=}\")\n",
    "    hist = model.fit_generator(\n",
    "        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n",
    "        steps_per_epoch = len(train_inputs) // batch_size,\n",
    "        epochs = n_epoch,\n",
    "        validation_data=Datagen(val_inputs, val_targets, batch_size),\n",
    "        validation_steps = len(val_inputs) // batch_size,\n",
    "        callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n",
    "        shuffle = False,\n",
    "        verbose = 1\n",
    "        )\n",
    "    return hist\n",
    "\n",
    "\n",
    "def lrs(epoch):\n",
    "    if epoch<35:\n",
    "        lr = learning_rate\n",
    "    elif epoch<50:\n",
    "        lr = learning_rate/10\n",
    "    else:\n",
    "        lr = learning_rate/100\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b19a15b-c138-4079-b885-5ad749d66117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (None, 512, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 01:34:00.021739: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-04 01:34:00.034040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-08-04 01:34:00.035980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-08-04 01:34:00.036035: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 01:34:00.852964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-04 01:34:00.853004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-08-04 01:34:00.853011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-08-04 01:34:00.856153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 38426 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:3b:00.0, compute capability: 8.0)\n",
      "2021-08-04 01:34:35.790901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-04 01:34:35.791515: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2100000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 01:34:41.990064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-08-04 01:34:43.065583: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\n",
      "2021-08-04 01:34:45.324834: W tensorflow/stream_executor/gpu/asm_compiler.cc:64] Running ptxas --version returned 32512\n",
      "2021-08-04 01:34:46.685722: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2021-08-04 01:34:46.730551: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-04 01:34:47.826156: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-04 01:34:47.951573: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120/3120 [==============================] - 458s 142ms/step - loss: 132.3460 - accuracy: 0.9411\n",
      "Epoch 2/5\n",
      "3120/3120 [==============================] - 441s 141ms/step - loss: 128.6318 - accuracy: 0.9578\n",
      "Epoch 3/5\n",
      "3120/3120 [==============================] - 441s 141ms/step - loss: 128.4012 - accuracy: 0.9583\n",
      "Epoch 4/5\n",
      "2963/3120 [===========================>..] - ETA: 22s - loss: 128.2353 - accuracy: 0.9585"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120/3120 [==============================] - 441s 141ms/step - loss: 128.0879 - accuracy: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd3d0289370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Unet()\n",
    "# print(model.summary())\n",
    "# d_train\n",
    "# l_train\n",
    "# d_validate\n",
    "# l_validate\n",
    "\n",
    "# learning_rate=0.0005\n",
    "# n_epoch=60\n",
    "# batch_size=32\n",
    "\n",
    "# lr_schedule = LearningRateScheduler(lrs)\n",
    "\n",
    "# #regressor\n",
    "# #model.compile(loss=\"mean_squared_error\", \n",
    "# #              optimizer=Adam(lr=learning_rate),\n",
    "# #              metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "#classifier\n",
    "learning_rate = 0.0001\n",
    "model.compile(loss=categorical_crossentropy, \n",
    "              optimizer=Adam(lr=learning_rate), \n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(d_train, l_train, batch_size = 2048, epochs=5)\n",
    "\n",
    "# hist = model_fit(model, d_train, l_train, d_validate, l_validate, n_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d82bbe92-ca0a-43f0-951f-e7e4d541425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(d_validate, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d067527-39aa-4153-8361-5ba5acdbb934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257446, 512, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089e8e747f754745a768a2215db83da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd7a93df130>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(l_validate[8000])\n",
    "ax[0].plot(preds[8000], alpha=0.5)\n",
    "ax[1].plot(d_validate[8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20173a8-6654-4bbd-8e7d-cbafa9445345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
